# üéØ VaultScan Evals ‚Äî Walkthrough Guide

> This guide walks you through the VaultScan evaluation framework ‚Äî what it does, how it works, and how to run it yourself.
>
> **Prerequisites:** Python venv activated, `.env` configured with Azure AI Foundry credentials

```powershell
# Activate environment before starting
.\venv\Scripts\Activate.ps1
```

---

## 1. üî¥ The Problem We're Solving

- üåç **Non-standard formats everywhere** ‚Äî Swiss SME annual reports, German HGB filings, UK Companies House, emerging market statements ‚Äî none follow US IFRS conventions. Every document is a snowflake.
- üìâ **No systematic accuracy measurement** ‚Äî Today, there's no way to answer "how accurate is our extraction pipeline?" with a number. Quality is anecdotal, not quantified.
- ‚è∞ **No degradation detection** ‚Äî When Azure Doc Intelligence updates, when prompts change, when new document formats appear ‚Äî we have no early warning system. Problems surface only when SMEs catch them manually.
- üí∏ **SME time is expensive** ‚Äî Credit analysts spend hours reviewing extractions that may already be correct, because there's no confidence signal telling them what actually needs attention.

> üí° What if you could measure accuracy per format, detect degradation automatically, and only route uncertain extractions to SMEs? That's exactly what this framework does.

---

## 2. üìê The Methodology: Dual Evaluation Strategy

### Offline Evaluation (Pre-production)
- üß™ **A/B test prompt strategies** ‚Äî Compare two extraction configurations with statistical significance testing
- üîÅ **Regression testing** ‚Äî Run against a golden dataset on every pipeline change; fail CI if accuracy drops below threshold
- üìä **Per-format accuracy breakdown** ‚Äî Know exactly which document formats are hardest (Swiss SME vs. German HGB vs. UK)

### Online Evaluation (Production)
- üì° **Real-time monitoring** ‚Äî Every extraction scored instantly, streamed to Application Insights
- üéØ **Confidence-based SME routing** ‚Äî Only route documents below confidence threshold to human review (reduce SME load by 40-60%)
- üìâ **Drift detection** ‚Äî Rolling window tracks accuracy over last 100 extractions; alerts on 15% degradation

### Five Custom Evaluators
| Evaluator | What it measures | Weight | Ground Truth Source |
|---|---|---|---|
| **Numerical Accuracy** | % deviation from SME-validated values (0-5 scale) | 30% | üè∑Ô∏è SME-labeled golden dataset (offline) |
| **Metric Completeness** | Were all 22 required metrics extracted? | 20% | üìã Required metrics list |
| **Math Consistency** | Do extracted values satisfy financial equations? (e.g. gross_profit = revenue ‚àí COGS) | 20% | üìê Accounting principles |
| **YoY Consistency** | Do year-over-year changes make financial sense? | 15% | üìÖ Prior year actuals |
| **Groundedness** | Are extracted values traceable to source document? | 15% | üìÑ Source document text |

> üí° **Key insight:** _We have **multiple layers of ground truth** in production ‚Äî the source document for grounding, accounting rules for math consistency, prior year data for trend validation. The only thing missing in production is SME-validated labels ‚Äî which is what the offline golden dataset provides. The feedback loop closes the gap: low-confidence extractions get SME review, corrections become golden dataset entries, and the system gets smarter over time._

> Let's see this running against synthetic data that mirrors your actual document formats.

---

## 3. üñ•Ô∏è Try It Yourself

### Step 1: Review Configuration (30 sec)

```powershell
python run_evaluation.py config
```

**What you'll see:**
- ‚úÖ Azure AI Foundry connection, model deployment, and evaluation thresholds
- ‚úÖ Numerical deviation bands: <0.1% = perfect, <1% = excellent, <5% = good, <10% = acceptable
- ‚úÖ YoY anomaly thresholds: >50% revenue swing, >30% asset swing, sign flips

---

### Step 2: Preview Document Formats (1 min)

```powershell
python run_evaluation.py generate --preview-all-formats
```

**What you'll see:**
- üìÑ **5 format templates** that mirror real-world client documents:
  - üá®üá≠ Swiss SME ‚Äî German-language labels, CHF, unique formatting
  - üá©üá™ German HGB ‚Äî Handelsgesetzbuch conventions, EUR
  - üá¨üáß UK Companies House ‚Äî Abbreviated accounts, GBP
  - üìù Unstructured narrative ‚Äî Metrics buried in prose paragraphs
  - üìä Table-based ‚Äî Clean tabular layout (the "easy" format)
- üéØ Each template has a difficulty rating ‚Äî this is why per-format accuracy matters

---

### Step 3: Generate Golden Dataset (1 min)

```powershell
python run_evaluation.py generate --size 20 --output data/golden_dataset.json
```

**What you'll see:**
- üèóÔ∏è Generates 20 synthetic documents with **known ground truth** values
- üí∞ Financially consistent: revenue ‚Üí COGS ‚Üí gross profit ‚Üí EBITDA ‚Üí net income all follow realistic ratios
- üìÖ Includes prior-year metrics with ~10% YoY variation for consistency checking
- üéØ Mix of difficulty levels (easy/medium/hard) across all 5 format types
- üìã This is what a real golden dataset looks like ‚Äî yours would use SME-validated real extractions

---

### Step 4: Online Evaluation ‚Äî Single Document (3 min)

```powershell
python run_evaluation.py online --demo --no-telemetry
```

**What you'll see:**
- üîç Extracts all **22 financial metrics** from a single document in real-time
- üìä Each metric gets: extracted value, confidence score, source location in document
- üßÆ **Five evaluator scores** computed instantly:
  - Numerical Accuracy: how close to ground truth? _(offline only ‚Äî needs SME-validated data)_
  - Completeness: any missing metrics?
  - **Math Consistency: do the numbers add up?** _(this is the key production signal ‚Äî checks 9 financial equations like gross_profit = revenue ‚àí COGS, balance sheet identity, ratio calculations)_
  - YoY Consistency: any suspicious year-over-year changes?
  - Groundedness: can we trace values back to the source?
- üö¶ **Routing decision**: HIGH confidence ‚Üí auto-approve | LOW confidence ‚Üí route to SME queue
- üí° **Multiple layers of ground truth in production** ‚Äî the source document itself (grounding), accounting principles (math consistency), prior year actuals (YoY). The only layer missing live is SME-validated labels ‚Äî which the feedback loop continuously builds.
- ‚ö° This runs on every extraction in production. SMEs only see what actually needs review.

> **FAQ: How does this work without ground truth?** Ground truth is derived from multiple sources. The source document is ground truth for grounding checks ‚Äî if the model says revenue is 15M, we verify that "15,000,000" appears in the document. Accounting principles are ground truth for math consistency ‚Äî gross profit must equal revenue minus COGS, the balance sheet must balance. Prior year actuals are ground truth for trend validation. The only thing not available in production is SME-validated "correct answers" ‚Äî and that's exactly what the feedback loop builds. Every SME correction on a routed extraction becomes a new golden dataset entry.

---

### Step 5: Offline Batch Evaluation (4 min)

```powershell
python run_evaluation.py offline --dataset data/golden_dataset.json --no-groundedness
```

> üí° We skip groundedness here for speed ‚Äî it requires an additional LLM call per document.

**What you'll see:**
- üìä **Per-format accuracy breakdown** ‚Äî Notice the difference between table-based (easy) and unstructured narrative (hard). This tells you exactly where to invest in prompt engineering.
- üìã **Per-metric accuracy** ‚Äî Some metrics extract reliably (revenue, total assets), others are consistently harder (working capital, interest coverage ratio)
- ‚ùå **Completeness gaps** ‚Äî Which metrics are most frequently missed, and in which format types?
- üéØ **Aggregate score** ‚Äî Single number (0-5 scale) to track over time; CI/CD threshold is 3.5

**If time permits ‚Äî A/B comparison:**

```powershell
python run_evaluation.py compare --config-a "baseline" --config-b "enhanced_prompts" --dataset data/golden_dataset.json
```

- üìà Side-by-side accuracy comparison with statistical significance testing
- This is how you can prove that a prompt change actually improves extraction ‚Äî not just on one document, but across all formats.

---

## 4. üí° Key Benefits & Value Proposition

| Capability | Business Value |
|---|---|
| üìä **Quantified accuracy per format** | Target prompt improvements where they matter most ‚Äî don't waste effort on formats that already work |
| üßÆ **Multi-layered ground truth** | Every evaluator has its own ground truth source ‚Äî source document, accounting rules, prior year actuals. No cold-start problem; works from day one |
| üö® **Automated regression detection** | Catch degradation from model updates, prompt changes, or new document types _before_ it reaches production |
| üéØ **Confidence-based SME routing** | Only route what needs human review ‚Äî reduce SME review load by 40-60% while maintaining quality |
| üîÑ **Continuous improvement loop** | SME corrections on routed documents feed back into the golden dataset ‚Äî the system gets smarter over time. Derived ground truth today ‚Üí labeled ground truth tomorrow |
| ‚úÖ **CI/CD quality gates** | No pipeline change ships if accuracy drops below threshold ‚Äî quality is enforced, not hoped for |

> **Bottom line:** This isn't just testing ‚Äî it's a quality infrastructure layer that sits on top of your existing pipeline. Azure Doc Intelligence ‚Üí LLM extraction ‚Üí **evaluation & routing** ‚Üí SME review. Every component you already have stays. We're adding the measurement and feedback loop. Each evaluator derives ground truth from a different source ‚Äî the source document, accounting principles, prior year actuals ‚Äî so you get immediate value from day one.

---

## 5. üöÄ Recommended Next Steps

### Immediate (Weeks 1-2)
- üìã **Collect 100-200 real SME-validated extractions** across format types to build the production golden dataset
- üìè **Baseline current pipeline accuracy** ‚Äî run offline evaluation against real data to get the starting numbers

### Short-term (Weeks 3-4)
- üß™ **A/B test prompt strategies** with statistical significance across Swiss SME, German HGB, UK formats
- üîß **Tune confidence thresholds** ‚Äî calibrate routing so SMEs get the right volume of reviews

### Medium-term (Month 2)
- üì° **Deploy online monitoring** with Application Insights dashboards and alerting
- üîÅ **Set up CI/CD quality gates** ‚Äî automated evaluation on every prompt or pipeline change
- üìä **Build format-specific prompt strategies** ‚Äî use per-format accuracy data to optimize where it matters

### Ongoing
- üîÑ **Feedback loop** ‚Äî SME corrections automatically enrich the golden dataset
- üìà **Track accuracy trends** over time ‚Äî demonstrate measurable improvement to stakeholders

> **The bottom line:** The question isn't whether extraction accuracy matters ‚Äî it's whether you can measure it. This framework gives you the numbers, the alerts, and the feedback loop to continuously improve.

---

## üìé Appendix: Quick Reference Commands

```powershell
# Activate environment
.\venv\Scripts\Activate.ps1

# Show configuration and validate setup
python run_evaluation.py config

# Preview document format templates
python run_evaluation.py generate --preview-all-formats

# Generate golden dataset (20 documents)
python run_evaluation.py generate --size 20 --output data/golden_dataset.json

# Online evaluation ‚Äî single document demo
python run_evaluation.py online --demo --no-telemetry

# Offline batch evaluation
python run_evaluation.py offline --dataset data/golden_dataset.json --no-groundedness

# Offline evaluation with groundedness (slower, requires LLM)
python run_evaluation.py offline --dataset data/golden_dataset.json

# A/B comparison of two configurations
python run_evaluation.py compare --config-a "baseline" --config-b "enhanced_prompts" --dataset data/golden_dataset.json

# Offline evaluation in CI mode (exits non-zero if below threshold)
python run_evaluation.py offline --dataset data/golden_dataset.json --ci --threshold 3.5
```
