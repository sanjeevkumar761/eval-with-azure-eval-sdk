from semantic_kernel import Kernel
from semantic_kernel.agents import ChatCompletionAgent
from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings
from semantic_kernel.functions.kernel_arguments import KernelArguments
from semantic_kernel.contents.chat_history import ChatHistory


class AgentEvaluator:
    def __init__(self):
        pass

    async def __call__(self, *, kernel, agent_name, instructions, input, output, **kwargs):
        """
        Evaluate the performance of an AI agent based on its output, input, and instructions.

        Args:
            kernel: The semantic kernel instance.
            agent_name: Name of the agent being evaluated.
            instructions: Instructions provided to the agent.
            input: Input given to the agent.
            output: Output generated by the agent.
            kwargs: Additional arguments.

        Returns:
            dict: Evaluation result containing the score and reason.
        """
        # Initialize chat history and settings
        history = ChatHistory()
        settings = AzureChatPromptExecutionSettings(service_id="utility")

        # Create the ChatCompletionAgent for evaluation
        agent_evaluator = ChatCompletionAgent(
            kernel=kernel,
            name="AgentEvaluator",
            instructions=f"""
                Agent Name: {agent_name}
                Instructions: {instructions}
                Input: {input}
                Output: {output}

                As a judge of AI agents' work, you need to evaluate the above work of the given Agent with Agent name 
                and give a score from 1 to 10 with a reason.
                For example:
                    Evaluation_Score: 5
                    Evaluation_Reason: The answer is not clear.
                The judging involves evaluating the Output of the agent based on the Input and Instructions given.
                Be very strict and honest in your evaluation.
                Always mention the name of the agent you evaluated.
            """,
            arguments=KernelArguments(settings=settings),
        )

        # Get the evaluation response
        response = await agent_evaluator.get_response(messages=instructions, history=history)

        # Return the evaluation result
        return {"eval_result": response.content}